{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import catboost as cat\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import log_loss, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing files\n",
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(data):\n",
    "    \n",
    "    '''\n",
    "    Function to input missing values based on the column object type\n",
    "    '''\n",
    "    \n",
    "    cols = list(data.columns)\n",
    "    for col in cols:\n",
    "        if data[col].dtype == 'int64' or data[col].dtype == 'float64':\n",
    "        \n",
    "            data[col] = data[col].fillna(data[col].mean())\n",
    "        \n",
    "        #elif data[col].dtype == 'O' or data[col].dtype == 'object':\n",
    "        #    data[col] = data[col].fillna(data[col].mode()[0])\n",
    "            \n",
    "        else:\n",
    "            data[col] = data[col].fillna(data[col].mode()[0])\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_encoding(traindata, testdata, *args):\n",
    "    \n",
    "    for _ in args:\n",
    "        train_enc = (traindata.groupby(_).size()) / len(traindata)\n",
    "        test_enc = (testdata.groupby(_).size()) / len(testdata)\n",
    "        \n",
    "        traindata[_] = traindata[_].apply(lambda x: train_enc[x])\n",
    "        testdata[_] = testdata[_].apply(lambda x: test_enc[x])\n",
    "    \n",
    "    return traindata, testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(traindata, testdata, *args):\n",
    "    \n",
    "    for ii in args:\n",
    "        traindata = pd.get_dummies(traindata, prefix=[ii], columns=[ii])\n",
    "        testdata = pd.get_dummies(testdata, prefix=[ii], columns=[ii])\n",
    "        \n",
    "    return traindata, testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(traindata, testdata, target, *args):\n",
    "    \n",
    "    labels = np.array(traindata[target])\n",
    "    \n",
    "    columns = []\n",
    "    for _ in args:\n",
    "        columns.append(_)\n",
    "        \n",
    "    traindata = traindata.drop(columns, axis=1)\n",
    "    traindata = traindata.drop(target, axis=1)\n",
    "    testdata = testdata.drop(columns, axis=1)\n",
    "        \n",
    "    return labels, traindata, testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target(data, cols):\n",
    "    kf = KFold(5)\n",
    "    a = pd.DataFrame()\n",
    "    for tr_ind, val_ind in kf.split(data):\n",
    "        X_tr, X_val= data.iloc[tr_ind].copy(), data.iloc[val_ind].copy()\n",
    "        for col in cols:\n",
    "            means = X_val[col].map(X_tr.groupby(col).CHURN.mean())\n",
    "            X_val[col + '_mean_target'] = means + 0.0001\n",
    "        a = pd.concat((a, X_val))\n",
    "    prior = CHURN.mean()\n",
    "    a.fillna(prior, inplace=True)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(traindata, testdata):\n",
    "    \n",
    "    cols = list(traindata.columns)\n",
    "    for _ in cols:\n",
    "        traindata[_] = np.where(traindata[_] == np.inf, 0, traindata[_])\n",
    "        testdata[_] = np.where(testdata[_] == np.inf, 0, testdata[_])\n",
    "        traindata[_] = np.where(traindata[_] == np.nan, 0, traindata[_])\n",
    "        testdata[_] = np.where(testdata[_] == np.nan, 0, testdata[_])\n",
    "        traindata[_] = np.where(traindata[_] == -np.inf, 0, traindata[_])\n",
    "        testdata[_] = np.where(testdata[_] == -np.inf, 0, testdata[_])\n",
    "        \n",
    "    return traindata, testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "target = train.CHURN.copy()\n",
    "df = pd.concat((train, test)).reset_index(drop=True)\n",
    "df = df.drop('user_id', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ZONE1', 'ZONE2', 'TIGO', 'MRG']\n",
    "\n",
    "labels, train1, test1 = drop_columns(df, test, 'CHURN', *cols)\n",
    "\n",
    "one_hot_cols = ['REGION', 'TENURE']\n",
    "\n",
    "train1, test1 = one_hot_encoding(train1, test1, *one_hot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 33)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train1.fillna(-999)\n",
    "test1 = test1.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_col = ['TOP_PACK']\n",
    "\n",
    "train1, test1 = freq_encoding(train1, test1, *freq_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1['REG*REV'] = train1['REGULARITY'] * train1['REVENUE']\n",
    "train1['REG/REV'] = train1['REGULARITY'] / train1['REVENUE']\n",
    "train1['LOG_REV'] = np.log(train1['REVENUE'])\n",
    "train1['REG*MUT'] = train1['REGULARITY'] * train1['MONTANT']\n",
    "train1['REG/MUT'] = train1['REGULARITY'] / train1['MONTANT']\n",
    "train1['LOG_MUT'] = np.log(train1['MONTANT'])\n",
    "train1['REG*NET'] = train1['REGULARITY'] * train1['ON_NET']\n",
    "train1['REG/NET'] = train1['REGULARITY'] / train1['ON_NET']\n",
    "train1['FREQ+FREQ'] = train1['FREQUENCE_RECH'] + train1['FREQUENCE']\n",
    "train1['FREQ-FREQ'] = train1['FREQUENCE_RECH'] - train1['FREQUENCE']\n",
    "train1['LOG_NET'] = np.log(train1['ON_NET'])\n",
    "train1['REV+DAT'] = train1['REVENUE'] + train1['DATA_VOLUME']\n",
    "train1['REV-DAT'] = train1['REVENUE'] - train1['DATA_VOLUME']\n",
    "train1['REV-DAT'] = train1['REVENUE'] - train1['DATA_VOLUME']\n",
    "train1['REV/DAT'] = train1['REVENUE'] / train1['DATA_VOLUME']\n",
    "train1['REV-ARP'] = train1['REVENUE'] - train1['ARPU_SEGMENT']\n",
    "train1['REV+ARP'] = train1['REVENUE'] + train1['ARPU_SEGMENT']\n",
    "train1['REV*ARP'] = train1['REVENUE'] * train1['ARPU_SEGMENT']\n",
    "train1['REV/ARP'] = train1['REVENUE'] / train1['ARPU_SEGMENT']\n",
    "train1['REV*FREQ'] = train1['REVENUE'] * train1['FREQUENCE']\n",
    "train1['REV/FREQ'] = train1['REVENUE'] / train1['FREQUENCE']\n",
    "train1['LOG_DAT'] = np.log(train1['DATA_VOLUME'])\n",
    "train1['LOG_ARP'] = np.log(train1['ARPU_SEGMENT'])\n",
    "\n",
    "test1['REG*REV'] = test1['REGULARITY'] * test1['REVENUE']\n",
    "test1['REG/REV'] = test1['REGULARITY'] / test1['REVENUE']\n",
    "test1['LOG_REV'] = np.log(test1['REVENUE'])\n",
    "test1['REG*MUT'] = test1['REGULARITY'] * test1['MONTANT']\n",
    "test1['REG/MUT'] = test1['REGULARITY'] / test1['MONTANT']\n",
    "test1['LOG_MUT'] = np.log(test1['MONTANT'])\n",
    "test1['REG*NET'] = test1['REGULARITY'] * test1['ON_NET']\n",
    "test1['REG/NET'] = test1['REGULARITY'] / test1['ON_NET']\n",
    "test1['FREQ+FREQ'] = test1['FREQUENCE_RECH'] + test1['FREQUENCE']\n",
    "test1['FREQ-FREQ'] = test1['FREQUENCE_RECH'] - test1['FREQUENCE']\n",
    "test1['LOG_NET'] = np.log(test1['ON_NET'])\n",
    "test1['REV+DAT'] = test1['REVENUE'] + test1['DATA_VOLUME']\n",
    "test1['REV-DAT'] = test1['REVENUE'] - test1['DATA_VOLUME']\n",
    "test1['REV-DAT'] = test1['REVENUE'] - test1['DATA_VOLUME']\n",
    "test1['REV/DAT'] = test1['REVENUE'] / test1['DATA_VOLUME']\n",
    "test1['REV-ARP'] = test1['REVENUE'] - test1['ARPU_SEGMENT']\n",
    "test1['REV+ARP'] = test1['REVENUE'] + test1['ARPU_SEGMENT']\n",
    "test1['REV*ARP'] = test1['REVENUE'] * test1['ARPU_SEGMENT']\n",
    "test1['REV/ARP'] = test1['REVENUE'] / test1['ARPU_SEGMENT']\n",
    "test1['REV*FREQ'] = test1['REVENUE'] * test1['FREQUENCE']\n",
    "test1['REV/FREQ'] = test1['REVENUE'] / test1['FREQUENCE']\n",
    "test1['LOG_DAT'] = np.log(test1['DATA_VOLUME'])\n",
    "test1['LOG_ARP'] = np.log(test1['ARPU_SEGMENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = process(train1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train1.fillna(-999)\n",
    "test1 = test1.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHURN = train.CHURN.copy()\n",
    "\n",
    "train2['CHURN'] = labels\n",
    "train1['CHURN'] = labels\n",
    "train2 = train1.copy()\n",
    "train2 = mean_target(train1, ['TOP_PACK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = train2[:ntrain].copy()\n",
    "CHURN = train3.CHURN.copy()\n",
    "train3.drop('CHURN', axis=1, inplace=True)\n",
    "\n",
    "test3 = train2[ntrain:].copy()\n",
    "test3.drop('CHURN', axis=1, inplace=True)\n",
    "test3 = test3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400000, 56), (100000, 56))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3.shape, test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
