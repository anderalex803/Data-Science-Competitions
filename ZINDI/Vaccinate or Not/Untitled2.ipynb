{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sklm\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "submissions = pd.read_csv('SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 4)\n",
      "(5177, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "safe_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.fillna('It was a good movie')\n",
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train['agreement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15178"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = list(train.safe_text.values) + list(test.safe_text.values)\n",
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n",
      "5177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['<user> <user> ... &amp; 4 a vaccine given 2 healthy peeps, FDA think just not worth the AE risk unfortunately.',\n",
       "       'Students starting school without whooping cough vaccinations <url> #scpick'],\n",
       "      dtype='<U152')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(combined[:10001])\n",
    "print(len(x_train))\n",
    "x_test = np.array(combined[10001:])\n",
    "print(len(x_test))\n",
    "x_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as nr\n",
    "import sklearn.model_selection as ms\n",
    "## Randomly sample cases to create independent training and test data\n",
    "nr.seed(9988)\n",
    "indx = range(np.array(x_train.shape[0]))\n",
    "indx = ms.train_test_split(indx, test_size = 0.2)\n",
    "x_train1 = x_train[indx[0]]\n",
    "y_train1 = np.ravel(y[indx[0]])\n",
    "x_test1 = x_train[indx[1]]\n",
    "y_test1 = np.ravel(y[indx[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,)\n",
      "(8000,)\n",
      "(2001,)\n",
      "(2001,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train1.shape)\n",
    "print(y_train1.shape)\n",
    "print(x_test1.shape)\n",
    "print(y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1,1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfv.fit(x_train)\n",
    "x_train_tfv =  tfv.transform(x_train1)\n",
    "x_test_tfv = tfv.transform(x_test1)\n",
    "\n",
    "\n",
    "test_enc = tfv.transform(test.safe_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4047)\n",
      "(8000,)\n",
      "(2001, 4047)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tfv.shape)\n",
    "print(y_train1.shape)\n",
    "print(x_test_tfv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000000    5868\n",
       "0.666667    3894\n",
       "0.333333     239\n",
       "Name: agreement, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.agreement.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.0319273\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tvalid_0's l2: 0.031877\n",
      "[3]\tvalid_0's l2: 0.0318279\n",
      "[4]\tvalid_0's l2: 0.0317809\n",
      "[5]\tvalid_0's l2: 0.0317341\n",
      "[6]\tvalid_0's l2: 0.0316878\n",
      "[7]\tvalid_0's l2: 0.0316419\n",
      "[8]\tvalid_0's l2: 0.0315971\n",
      "[9]\tvalid_0's l2: 0.0315545\n",
      "[10]\tvalid_0's l2: 0.0315118\n",
      "[11]\tvalid_0's l2: 0.0314716\n",
      "[12]\tvalid_0's l2: 0.0314337\n",
      "[13]\tvalid_0's l2: 0.0313946\n",
      "[14]\tvalid_0's l2: 0.0313587\n",
      "[15]\tvalid_0's l2: 0.0313213\n",
      "[16]\tvalid_0's l2: 0.0312858\n",
      "[17]\tvalid_0's l2: 0.0312508\n",
      "[18]\tvalid_0's l2: 0.0312146\n",
      "[19]\tvalid_0's l2: 0.0311819\n",
      "[20]\tvalid_0's l2: 0.0311502\n",
      "[21]\tvalid_0's l2: 0.0311172\n",
      "[22]\tvalid_0's l2: 0.0310863\n",
      "[23]\tvalid_0's l2: 0.0310558\n",
      "[24]\tvalid_0's l2: 0.0310257\n",
      "[25]\tvalid_0's l2: 0.030995\n",
      "[26]\tvalid_0's l2: 0.030966\n",
      "[27]\tvalid_0's l2: 0.0309376\n",
      "[28]\tvalid_0's l2: 0.0309083\n",
      "[29]\tvalid_0's l2: 0.0308789\n",
      "[30]\tvalid_0's l2: 0.0308524\n",
      "[31]\tvalid_0's l2: 0.0308231\n",
      "[32]\tvalid_0's l2: 0.0307975\n",
      "[33]\tvalid_0's l2: 0.0307723\n",
      "[34]\tvalid_0's l2: 0.0307488\n",
      "[35]\tvalid_0's l2: 0.030722\n",
      "[36]\tvalid_0's l2: 0.0306995\n",
      "[37]\tvalid_0's l2: 0.0306738\n",
      "[38]\tvalid_0's l2: 0.0306525\n",
      "[39]\tvalid_0's l2: 0.0306291\n",
      "[40]\tvalid_0's l2: 0.0306089\n",
      "[41]\tvalid_0's l2: 0.0305879\n",
      "[42]\tvalid_0's l2: 0.030568\n",
      "[43]\tvalid_0's l2: 0.0305517\n",
      "[44]\tvalid_0's l2: 0.0305321\n",
      "[45]\tvalid_0's l2: 0.0305143\n",
      "[46]\tvalid_0's l2: 0.0304965\n",
      "[47]\tvalid_0's l2: 0.0304798\n",
      "[48]\tvalid_0's l2: 0.0304648\n",
      "[49]\tvalid_0's l2: 0.0304464\n",
      "[50]\tvalid_0's l2: 0.0304265\n",
      "[51]\tvalid_0's l2: 0.0304073\n",
      "[52]\tvalid_0's l2: 0.0303908\n",
      "[53]\tvalid_0's l2: 0.0303708\n",
      "[54]\tvalid_0's l2: 0.0303518\n",
      "[55]\tvalid_0's l2: 0.0303337\n",
      "[56]\tvalid_0's l2: 0.0303186\n",
      "[57]\tvalid_0's l2: 0.030303\n",
      "[58]\tvalid_0's l2: 0.0302871\n",
      "[59]\tvalid_0's l2: 0.0302715\n",
      "[60]\tvalid_0's l2: 0.0302546\n",
      "[61]\tvalid_0's l2: 0.0302396\n",
      "[62]\tvalid_0's l2: 0.0302258\n",
      "[63]\tvalid_0's l2: 0.0302091\n",
      "[64]\tvalid_0's l2: 0.0301953\n",
      "[65]\tvalid_0's l2: 0.0301812\n",
      "[66]\tvalid_0's l2: 0.0301684\n",
      "[67]\tvalid_0's l2: 0.0301578\n",
      "[68]\tvalid_0's l2: 0.0301461\n",
      "[69]\tvalid_0's l2: 0.0301315\n",
      "[70]\tvalid_0's l2: 0.0301225\n",
      "[71]\tvalid_0's l2: 0.0301128\n",
      "[72]\tvalid_0's l2: 0.0301005\n",
      "[73]\tvalid_0's l2: 0.030092\n",
      "[74]\tvalid_0's l2: 0.0300795\n",
      "[75]\tvalid_0's l2: 0.0300688\n",
      "[76]\tvalid_0's l2: 0.0300565\n",
      "[77]\tvalid_0's l2: 0.0300487\n",
      "[78]\tvalid_0's l2: 0.0300421\n",
      "[79]\tvalid_0's l2: 0.0300319\n",
      "[80]\tvalid_0's l2: 0.0300226\n",
      "[81]\tvalid_0's l2: 0.0300145\n",
      "[82]\tvalid_0's l2: 0.0300062\n",
      "[83]\tvalid_0's l2: 0.029997\n",
      "[84]\tvalid_0's l2: 0.0299918\n",
      "[85]\tvalid_0's l2: 0.0299855\n",
      "[86]\tvalid_0's l2: 0.029975\n",
      "[87]\tvalid_0's l2: 0.0299714\n",
      "[88]\tvalid_0's l2: 0.0299624\n",
      "[89]\tvalid_0's l2: 0.0299566\n",
      "[90]\tvalid_0's l2: 0.0299517\n",
      "[91]\tvalid_0's l2: 0.0299403\n",
      "[92]\tvalid_0's l2: 0.0299341\n",
      "[93]\tvalid_0's l2: 0.0299307\n",
      "[94]\tvalid_0's l2: 0.0299243\n",
      "[95]\tvalid_0's l2: 0.0299171\n",
      "[96]\tvalid_0's l2: 0.0299095\n",
      "[97]\tvalid_0's l2: 0.0299031\n",
      "[98]\tvalid_0's l2: 0.029901\n",
      "[99]\tvalid_0's l2: 0.0298946\n",
      "[100]\tvalid_0's l2: 0.0298881\n",
      "[101]\tvalid_0's l2: 0.0298844\n",
      "[102]\tvalid_0's l2: 0.0298833\n",
      "[103]\tvalid_0's l2: 0.0298787\n",
      "[104]\tvalid_0's l2: 0.0298694\n",
      "[105]\tvalid_0's l2: 0.0298665\n",
      "[106]\tvalid_0's l2: 0.0298634\n",
      "[107]\tvalid_0's l2: 0.0298626\n",
      "[108]\tvalid_0's l2: 0.0298609\n",
      "[109]\tvalid_0's l2: 0.0298576\n",
      "[110]\tvalid_0's l2: 0.0298562\n",
      "[111]\tvalid_0's l2: 0.0298483\n",
      "[112]\tvalid_0's l2: 0.0298503\n",
      "[113]\tvalid_0's l2: 0.0298428\n",
      "[114]\tvalid_0's l2: 0.02984\n",
      "[115]\tvalid_0's l2: 0.0298369\n",
      "[116]\tvalid_0's l2: 0.029834\n",
      "[117]\tvalid_0's l2: 0.0298335\n",
      "[118]\tvalid_0's l2: 0.0298302\n",
      "[119]\tvalid_0's l2: 0.0298282\n",
      "[120]\tvalid_0's l2: 0.0298249\n",
      "[121]\tvalid_0's l2: 0.0298232\n",
      "[122]\tvalid_0's l2: 0.0298225\n",
      "[123]\tvalid_0's l2: 0.0298205\n",
      "[124]\tvalid_0's l2: 0.0298191\n",
      "[125]\tvalid_0's l2: 0.0298173\n",
      "[126]\tvalid_0's l2: 0.0298145\n",
      "[127]\tvalid_0's l2: 0.0298112\n",
      "[128]\tvalid_0's l2: 0.0298122\n",
      "[129]\tvalid_0's l2: 0.0298129\n",
      "[130]\tvalid_0's l2: 0.0298127\n",
      "[131]\tvalid_0's l2: 0.0298127\n",
      "[132]\tvalid_0's l2: 0.0298124\n",
      "[133]\tvalid_0's l2: 0.0298123\n",
      "[134]\tvalid_0's l2: 0.0298126\n",
      "[135]\tvalid_0's l2: 0.0298115\n",
      "[136]\tvalid_0's l2: 0.0298116\n",
      "[137]\tvalid_0's l2: 0.0298078\n",
      "[138]\tvalid_0's l2: 0.0298083\n",
      "[139]\tvalid_0's l2: 0.0298085\n",
      "[140]\tvalid_0's l2: 0.0298072\n",
      "[141]\tvalid_0's l2: 0.0298065\n",
      "[142]\tvalid_0's l2: 0.0298083\n",
      "[143]\tvalid_0's l2: 0.0298038\n",
      "[144]\tvalid_0's l2: 0.0298033\n",
      "[145]\tvalid_0's l2: 0.0298025\n",
      "[146]\tvalid_0's l2: 0.0298017\n",
      "[147]\tvalid_0's l2: 0.0298043\n",
      "[148]\tvalid_0's l2: 0.0298031\n",
      "[149]\tvalid_0's l2: 0.0298039\n",
      "[150]\tvalid_0's l2: 0.0298015\n",
      "[151]\tvalid_0's l2: 0.0298023\n",
      "[152]\tvalid_0's l2: 0.0298033\n",
      "[153]\tvalid_0's l2: 0.0298036\n",
      "[154]\tvalid_0's l2: 0.0298045\n",
      "[155]\tvalid_0's l2: 0.0297992\n",
      "[156]\tvalid_0's l2: 0.0297983\n",
      "[157]\tvalid_0's l2: 0.0298019\n",
      "[158]\tvalid_0's l2: 0.0297972\n",
      "[159]\tvalid_0's l2: 0.0297957\n",
      "[160]\tvalid_0's l2: 0.0297968\n",
      "[161]\tvalid_0's l2: 0.0298002\n",
      "[162]\tvalid_0's l2: 0.0298028\n",
      "[163]\tvalid_0's l2: 0.0298039\n",
      "[164]\tvalid_0's l2: 0.0298067\n",
      "[165]\tvalid_0's l2: 0.0298063\n",
      "[166]\tvalid_0's l2: 0.0298068\n",
      "[167]\tvalid_0's l2: 0.0298101\n",
      "[168]\tvalid_0's l2: 0.0298096\n",
      "[169]\tvalid_0's l2: 0.0298141\n",
      "[170]\tvalid_0's l2: 0.0298174\n",
      "[171]\tvalid_0's l2: 0.029817\n",
      "[172]\tvalid_0's l2: 0.0298187\n",
      "[173]\tvalid_0's l2: 0.0298202\n",
      "[174]\tvalid_0's l2: 0.0298197\n",
      "[175]\tvalid_0's l2: 0.0298223\n",
      "[176]\tvalid_0's l2: 0.0298255\n",
      "[177]\tvalid_0's l2: 0.0298265\n",
      "[178]\tvalid_0's l2: 0.0298278\n",
      "[179]\tvalid_0's l2: 0.0298308\n",
      "[180]\tvalid_0's l2: 0.0298344\n",
      "[181]\tvalid_0's l2: 0.0298353\n",
      "[182]\tvalid_0's l2: 0.0298346\n",
      "[183]\tvalid_0's l2: 0.0298356\n",
      "[184]\tvalid_0's l2: 0.0298357\n",
      "[185]\tvalid_0's l2: 0.0298343\n",
      "[186]\tvalid_0's l2: 0.0298337\n",
      "[187]\tvalid_0's l2: 0.0298331\n",
      "[188]\tvalid_0's l2: 0.029833\n",
      "[189]\tvalid_0's l2: 0.0298374\n",
      "[190]\tvalid_0's l2: 0.0298384\n",
      "[191]\tvalid_0's l2: 0.0298406\n",
      "[192]\tvalid_0's l2: 0.0298417\n",
      "[193]\tvalid_0's l2: 0.0298456\n",
      "[194]\tvalid_0's l2: 0.0298481\n",
      "[195]\tvalid_0's l2: 0.0298521\n",
      "[196]\tvalid_0's l2: 0.0298553\n",
      "[197]\tvalid_0's l2: 0.0298571\n",
      "[198]\tvalid_0's l2: 0.0298552\n",
      "[199]\tvalid_0's l2: 0.0298563\n",
      "[200]\tvalid_0's l2: 0.029858\n",
      "[201]\tvalid_0's l2: 0.0298609\n",
      "[202]\tvalid_0's l2: 0.0298613\n",
      "[203]\tvalid_0's l2: 0.0298613\n",
      "[204]\tvalid_0's l2: 0.029865\n",
      "[205]\tvalid_0's l2: 0.0298653\n",
      "[206]\tvalid_0's l2: 0.0298674\n",
      "[207]\tvalid_0's l2: 0.0298693\n",
      "[208]\tvalid_0's l2: 0.0298729\n",
      "[209]\tvalid_0's l2: 0.0298758\n",
      "Early stopping, best iteration is:\n",
      "[159]\tvalid_0's l2: 0.0297957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "       importance_type='split', learning_rate=0.01, max_depth=50,\n",
       "       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "       n_estimators=3000, n_jobs=-1, num_leaves=31, objective=None,\n",
       "       random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.01, n_estimators=3000, max_depth=50)\n",
    "eval_set = [(x_test_tfv, y_test1)]\n",
    "lg.fit(x_train_tfv, y_train1, eval_set=eval_set, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17261433479048996"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = lg.predict(x_test_tfv)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test1, predictions) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc = tfv.transform(x_test)\n",
    "pred = lg.predict(test_enc)\n",
    "test['agreement'] = pred\n",
    "test.to_csv('test1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=1,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=1,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfv.fit(train.safe_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8de80e60027e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magreement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m-> 1361\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfv.fit(train.agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
